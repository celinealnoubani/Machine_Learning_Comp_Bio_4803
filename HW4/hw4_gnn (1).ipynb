{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msGqrVs7-u4u"
   },
   "source": [
    "## Coding assignment: Graph Neural Networks (GNN)\n",
    "\n",
    "Graph structures are ubiquitous in various domains, from social networks to molecular interactions. Understanding these complex relationships requires advanced analytical tools, and Graph Neural Networks (GNNs) provide a powerful framework for extracting meaningful insights from graph-structured data.\n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "- Gain a foundational understanding of GNNs and their underlying principles,\n",
    "- Explore their applications in graph analysis, and\n",
    "- Implement GNNs using state-of-the-art deep learning frameworks.\n",
    "-\n",
    "By the end of this assignment, you will have acquired both theoretical knowledge and hands-on experience in applying GNNs to real-world graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfqyBI4Z-u4v"
   },
   "source": [
    "## Environment Setup\n",
    "For a seamless execution of this notebook, ensure your Python environment is properly set up. Here's what you'll need:\n",
    "\n",
    "Python Version: We recommend using Python 3.8 or higher.\n",
    "\n",
    "Required Packages: Install the following libraries to delve into GNNs:\n",
    "\n",
    "```\n",
    "torch\n",
    "torch_geometric\n",
    "torch_scatter\n",
    "torch_sparse\n",
    "torchmetrics\n",
    "networkx\n",
    "numpy\n",
    "jupyter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GD6dUEAc-u4w",
    "outputId": "e241f563-1cfc-430f-9e33-d4e6d8986ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_TG8aPC-u4w"
   },
   "source": [
    "In this assignment, we will utilize the **CLUSTER** dataset from the GNNBenchmarkDataset, as introduced in the paper Benchmarking Graph Neural Networks. This dataset is a subset of the Stochastic Block Model (SBM) datasets, which focus on node-level graph pattern recognition tasks, originally explored by Scarselli et al. (2009). Specifically, it addresses the following tasks:\n",
    "- Graph Pattern Recognition (PATTERN)\n",
    "- Semi-supervised Graph Clustering (CLUSTER)\n",
    "The Stochastic Block Model (SBM), as described by Abbe (2017), serves as the foundation of these datasets. SBM is widely used for modeling community structures in social networks, where intra- and inter-community connections are probabilistically controlled. In particular:\n",
    "\n",
    "- Two nodes within the same community are connected with probability p.\n",
    "- Nodes belonging to different communities are connected with probability q, which acts as a noise parameter.\n",
    "\n",
    "The CLUSTER dataset has the following properties:\n",
    "\n",
    "- Each node is represented by a 7-dimensional feature vector.\n",
    "- The dataset contains 6 distinct node classes.\n",
    "- The primary learning objective is multi-class classification at the node level.\n",
    "\n",
    "This dataset provides a challenging yet insightful benchmark for evaluating the performance of Graph Neural Networks in community detection and clustering tasks. We will start by loading the dataset and take a look at the graphs in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzcVU9Ej-u4w",
    "outputId": "015c6568-39fe-4d81-87f3-2c3ccd71b867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 10000\n",
      "Validation dataset size: 1000\n",
      "Graph 0:\n",
      "  - Number of nodes: 117\n",
      "  - Number of edges: 4104\n",
      "  - Node features shape: torch.Size([117, 7])\n",
      "  - Edge index shape: torch.Size([2, 4104])\n",
      "  - Labels shape: torch.Size([117])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Cluster dataset\n",
    "data_root = './data'\n",
    "dataset_train = GNNBenchmarkDataset(root=data_root, name='CLUSTER', split='train')\n",
    "dataset_val = GNNBenchmarkDataset(root=data_root, name='CLUSTER', split='val')\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f'Training dataset size: {len(dataset_train)}')\n",
    "print(f'Validation dataset size: {len(dataset_val)}')\n",
    "\n",
    "# Print a sample graph's details\n",
    "def print_graph_info(graph, index):\n",
    "    print(f'Graph {index}:')\n",
    "    print(f'  - Number of nodes: {graph.num_nodes}')\n",
    "    print(f'  - Number of edges: {graph.num_edges}')\n",
    "    print(f'  - Node features shape: {graph.x.shape}')\n",
    "    print(f'  - Edge index shape: {graph.edge_index.shape}')\n",
    "    print(f'  - Labels shape: {graph.y.shape}')\n",
    "    print('-' * 40)\n",
    "\n",
    "# Display information about the first few graphs\n",
    "for i in range(min(1, len(dataset_train))):\n",
    "    print_graph_info(dataset_train[i], i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ9KAMcl-u4x"
   },
   "source": [
    "Then we use torch_geometric's data loader class to wrap the dataset to batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JWxwNC0X-u4x",
    "outputId": "6f668f66-7849-47dd-c4a4-9706afcad46d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched graph:\n",
      "  - Batch size: 32\n",
      "  - Total nodes in batch: 3561\n",
      "  - Total edges in batch: 123310\n",
      "  - Labels shape: torch.Size([3561])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display batched graph information\n",
    "for batch in dataloader_train:\n",
    "    print('Batched graph:')\n",
    "    print(f'  - Batch size: {batch_size}')\n",
    "    print(f'  - Total nodes in batch: {batch.x.shape[0]}')\n",
    "    print(f'  - Total edges in batch: {batch.edge_index.shape[1]}')\n",
    "    print(f'  - Labels shape: {batch.y.shape}')\n",
    "    break  # Only show one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omHBOajy-u4x"
   },
   "source": [
    "## Task 1: MLP for node classification\n",
    "\n",
    "To start with, we will build an MLP model as a baseline. The MLP should directly take the node features as input and output the predictions for each class. In this task, you need to complete the MLP model and the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMju7PmR-u4x",
    "outputId": "5a4c5521-850c-413b-bf8e-1d6c209e515b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class MLPNodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        #####\n",
    "        # TODO:\n",
    "        super(MLPNodeClassifier, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        #####\n",
    "        # TODO:\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7TTiYmk-u4x"
   },
   "source": [
    "The training and evaluation functions for the MLP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B7cGhOmf-u4x"
   },
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device=device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch.x)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "            #####\n",
    "    # return the accuracy\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader_train, dataloader_val, epochs=50, lr=0.001, patience=5, device=device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0 # number of correct predicted nodes\n",
    "        total = 0 # number of nodes\n",
    "\n",
    "        for batch in dataloader_train:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch.x)\n",
    "            loss = criterion(outputs, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch.y.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "            #####\n",
    "\n",
    "        train_acc = correct / total if total > 0 else 0\n",
    "        val_acc = evaluate(model, dataloader_val, device)\n",
    "        print(f'Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_ckpt = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "    return best_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zuj2UqZi-u4x"
   },
   "source": [
    "We will train the MLP for the node classification task. There are several hyperparameters that might need your attention, including the batch size, hidden dimension of the MLP, number of epochs, learning rate, and early stop patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSK46-eb-u4x",
    "outputId": "5a4ecd02-87c5-4ac1-eb67-e66e7b167ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPNodeClassifier(\n",
      "  (fc1): Linear(in_features=7, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n",
      "Epoch 1: Loss=2005784.9551, Train Acc=0.2088, Val Acc=0.2133\n",
      "Epoch 2: Loss=1995162.7075, Train Acc=0.2095, Val Acc=0.2096\n",
      "Epoch 3: Loss=1994486.4846, Train Acc=0.2096, Val Acc=0.2096\n",
      "Epoch 4: Loss=1993541.8384, Train Acc=0.2093, Val Acc=0.2133\n",
      "Epoch 5: Loss=1993437.9511, Train Acc=0.2080, Val Acc=0.2072\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_dim = dataset_train.num_node_features\n",
    "hidden_dim = 32\n",
    "output_dim = dataset_train.num_classes\n",
    "\n",
    "model = MLPNodeClassifier(input_dim, hidden_dim, output_dim)\n",
    "print(model)\n",
    "\n",
    "# Train the model\n",
    "best_ckpt = train(model, dataloader_train, dataloader_val, epochs=5, lr=1e-2, patience=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx_bOyRO-u4y"
   },
   "source": [
    "After the training, we will make predictions on the test set and save the prediction results. The saved predictions will be used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UYl8jLUp-u4y"
   },
   "outputs": [],
   "source": [
    "dataset_test = GNNBenchmarkDataset(root=data_root, name='CLUSTER', split='test')\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test set predictions and save results\n",
    "def predict(model, dataloader, filename='predictions.txt', device=device):\n",
    "    model.eval()\n",
    "    predictions = [] # a list of predicted labels\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch.x)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            #####\n",
    "\n",
    "    return predictions\n",
    "\n",
    "model.load_state_dict(best_ckpt)\n",
    "predictions = predict(model, dataloader_test)\n",
    "np.savetxt('predictions_mlp_cluster.txt', predictions, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EyO-uwB-u4y"
   },
   "source": [
    "## Task 2: GCN for node classification\n",
    "\n",
    "Next we will leverage the graph convolutional layers to construct a GNN model for the node classification task. You can check pyg's [documentation](https://pytorch-geometric.readthedocs.io/en/latest/) to learn the usage of the `GCNConv` module and use it to build a GNN model. In the following, you need to complete the `GNNNodeClassifier` class as well as the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "HZfvkKmN-u4y"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "# Define a GNN model using GCNConv\n",
    "class GNNNodeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers=2):\n",
    "        #####\n",
    "        # TODO:\n",
    "        super(GNNNodeClassifier, self).__init__()\n",
    "        dims = [input_dim] + [hidden_dim] * (num_gcn_layers - 1) + [output_dim]\n",
    "        self.convs = nn.ModuleList(\n",
    "            [GCNConv(dims[i], dims[i + 1]) for i in range(len(dims) - 1)]\n",
    "        )\n",
    "        #####\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #####\n",
    "        # TODO:\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "        #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "TBo-NfzS-u4y"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader_train, dataloader_val, epochs=50, lr=0.001, patience=5, device=device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in dataloader_train:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(outputs, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * batch.y.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "            #####\n",
    "\n",
    "        train_acc = correct / total\n",
    "        val_acc = evaluate(model, dataloader_val, device)\n",
    "        scheduler.step(val_acc)\n",
    "        print(f'Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device=device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch.x, batch.edge_index)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "            #####\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy5lgJc8-u4y"
   },
   "source": [
    "Then we can train the GCN model for the node classification. You should carefully choose the hyperparameters: batch size, hidden dimension, number of GCN layers, number of epochs, learning rate, and early stop patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WU2KOlT3-u4y",
    "outputId": "80a4daa1-8428-4888-fe0e-5a50365321ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNNodeClassifier(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(7, 128)\n",
      "    (1): GCNConv(128, 6)\n",
      "  )\n",
      ")\n",
      "Number of trainable params: 1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/calnoubani3/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=2008820.6207, Train Acc=0.3038, Val Acc=0.4442\n",
      "Epoch 2: Loss=1759370.1928, Train Acc=0.4398, Val Acc=0.4139\n",
      "Epoch 3: Loss=1682736.5758, Train Acc=0.4541, Val Acc=0.4610\n",
      "Epoch 4: Loss=1660280.6751, Train Acc=0.4581, Val Acc=0.4562\n",
      "Epoch 5: Loss=1652592.4957, Train Acc=0.4595, Val Acc=0.4603\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_dim = dataset_train.num_node_features\n",
    "hidden_dim = 128\n",
    "output_dim = dataset_train.num_classes\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNNNodeClassifier(input_dim, hidden_dim, output_dim, num_gcn_layers=2)\n",
    "print(model)\n",
    "print(f'Number of trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader_train, dataloader_val, epochs=5, lr=1e-2, patience=3, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtJG6rma-u4y"
   },
   "source": [
    "After training, we can make predictions on the test set and save the results. The results will be used for the grading of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SOS65DqO-u4y"
   },
   "outputs": [],
   "source": [
    "# Test set predictions and save results\n",
    "def predict(model, dataloader, filename='gnn_predictions.txt', device=device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch.x, batch.edge_index)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            #####\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predictions = predict(model, dataloader_test)\n",
    "np.savetxt('predictions_gcn_cluster.txt', predictions, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsgOUUCb-u4z"
   },
   "source": [
    "## Graph classification task\n",
    "\n",
    "In this section, we explore graph classification using Graph Neural Networks (GNNs). Unlike node classification, which focuses on predicting labels for individual nodes within a graph, graph classification aims to assign labels to entire graphs based on their structural and feature-based attributes. The primary challenge lies in effectively embedding entire graphs into a feature space where they become linearly separable for classification tasks.\n",
    "\n",
    "One notable application of graph classification is the representation of image data as graphs, an approach demonstrated in super-pixel datasets. These datasets provide a novel way to transform traditional image classification tasks into graph learning problems. Prominent image datasets such as MNIST and CIFAR10 have been adapted into graph structures using this methodology. The motivation for utilizing these datasets is twofold:\n",
    "\n",
    "- Benchmarking and Sanity-Checking – These datasets serve as standard benchmarks for evaluating the performance of GNN architectures. Most GNN models are expected to achieve near-perfect accuracy on MNIST and competitive performance on CIFAR10.\n",
    "- Extending Image-Based Learning to Graphs – Super-pixel representations offer valuable insights into how conventional image datasets can be leveraged for graph-based learning and analysis.\n",
    "\n",
    "### CIFAR10 Super-Pixel Dataset\n",
    "In this assignment, we will work with the CIFAR10 super-pixel dataset for a graph classification task. The CIFAR10 images are transformed into graphs using super-pixel segmentation, where each super-pixel represents a small, homogeneous region of the image. This transformation is performed using the Simple Linear Iterative Clustering (SLIC) algorithm, introduced by Achanta et al. (2012).\n",
    "\n",
    "By leveraging super-pixel representations, we can analyze the effectiveness of GNNs in graph classification while drawing connections between traditional computer vision tasks and graph-based learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IUpEQMJ1-u4z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.pyg.org/datasets/benchmarking-gnns/CIFAR10_v2.zip\n",
      "Extracting data/GNNBenchmark/CIFAR10/raw/CIFAR10_v2.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 45000\n",
      "Validation dataset size: 5000\n",
      "Graph 0:\n",
      "  - Number of nodes: 110\n",
      "  - Number of edges: 880\n",
      "  - Node features shape: torch.Size([110, 3])\n",
      "  - Edge index shape: torch.Size([2, 880])\n",
      "  - Labels shape: torch.Size([1])\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "data_root = './data/GNNBenchmark'\n",
    "dataset_train = GNNBenchmarkDataset(root=data_root, name='CIFAR10', split='train')\n",
    "dataset_val = GNNBenchmarkDataset(root=data_root, name='CIFAR10', split='val')\n",
    "\n",
    "print(f'Training dataset size: {len(dataset_train)}')\n",
    "print(f'Validation dataset size: {len(dataset_val)}')\n",
    "print_graph_info(dataset_train[0], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XF0xrqM-u4z"
   },
   "source": [
    "## Task 3: GCN for graph classification\n",
    "\n",
    "In this task, you need to build a GNN model using the `GCNConv` module for the graph classification task. Note that in order to do graph classification, you need to get the graph embedding by pooling the node embeddings. You can refer to pyg's [documentation](https://pytorch-geometric.readthedocs.io/en/latest/) for different pooling functions (e.g., mean pooling, max pooling, sum pooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "y8Epk8eO-u4z"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "# Define a GCN model for graph classification\n",
    "class GCNGraphClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers=2, embedding_dim=512):\n",
    "        #####\n",
    "        # TODO:\n",
    "        super(GCNGraphClassifier, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(embedding_dim, hidden_dim))\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        for _ in range(num_gcn_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        #####\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        #####\n",
    "        # TODO:\n",
    "        x = self.embedding(x)\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "        #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vMxwAREg-u4z"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, dataloader_train, dataloader_val, epochs=50, lr=0.001, patience=5, device=device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in dataloader_train:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "            # Fix for y shape mismatch: get graph level labels\n",
    "            if batch.y.shape[0] != out.shape[0]:\n",
    "                graph_y = []\n",
    "                for i in range(out.shape[0]):\n",
    "                    graph_indices = (batch.batch == i).nonzero(as_tuple=True)[0]\n",
    "                    graph_y.append(batch.y[graph_indices[0]])\n",
    "                target = torch.tensor(graph_y, device=device)\n",
    "            else:\n",
    "                target = batch.y\n",
    "\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            #####\n",
    "\n",
    "        train_acc = correct / total\n",
    "        val_acc = evaluate(model, dataloader_val, device)\n",
    "        print(f'Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device=device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "\n",
    "            # Fix for y shape mismatch: get graph level labels\n",
    "            if batch.y.shape[0] != out.shape[0]:\n",
    "                graph_y = []\n",
    "                for i in range(out.shape[0]):\n",
    "                    graph_indices = (batch.batch == i).nonzero(as_tuple=True)[0]\n",
    "                    graph_y.append(batch.y[graph_indices[0]])\n",
    "                target = torch.tensor(graph_y, device=device)\n",
    "            else:\n",
    "                target = batch.y\n",
    "\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            #####\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWDwa5Jn-u4z"
   },
   "source": [
    "Train the GCN model for graph classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "id": "a2WZidcx-u4z",
    "outputId": "6305730c-7e4d-464b-df74-cbebd6cc2671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNGraphClassifier(\n",
      "  (embedding): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(512, 256)\n",
      "    (1-3): 3 x GCNConv(256, 256)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0-3): 4 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of trainable params: 600842\n",
      "Epoch 1: Loss=88614.7512, Train Acc=0.2780, Val Acc=0.3176\n",
      "Epoch 2: Loss=84339.7491, Train Acc=0.3138, Val Acc=0.3144\n",
      "Epoch 3: Loss=82259.9726, Train Acc=0.3347, Val Acc=0.3444\n",
      "Epoch 4: Loss=80980.4687, Train Acc=0.3414, Val Acc=0.3526\n",
      "Epoch 5: Loss=79785.0954, Train Acc=0.3574, Val Acc=0.3674\n",
      "Epoch 6: Loss=78914.0479, Train Acc=0.3633, Val Acc=0.3778\n",
      "Epoch 7: Loss=78189.0324, Train Acc=0.3712, Val Acc=0.3926\n",
      "Epoch 8: Loss=77228.1822, Train Acc=0.3775, Val Acc=0.3944\n",
      "Epoch 9: Loss=76616.2627, Train Acc=0.3820, Val Acc=0.3994\n",
      "Epoch 10: Loss=76195.1920, Train Acc=0.3906, Val Acc=0.4092\n",
      "Epoch 11: Loss=75686.3807, Train Acc=0.3912, Val Acc=0.3964\n",
      "Epoch 12: Loss=75339.8786, Train Acc=0.3972, Val Acc=0.4248\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 60\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize models\n",
    "input_dim = dataset_train.num_node_features\n",
    "hidden_dim = 256\n",
    "embedding_dim = 512\n",
    "output_dim = dataset_train.num_classes\n",
    "\n",
    "gcn_model = GCNGraphClassifier(input_dim, hidden_dim, output_dim, num_gcn_layers=4)\n",
    "gcn_model.to(device)\n",
    "print(gcn_model)\n",
    "print(f'Number of trainable params: {sum(p.numel() for p in gcn_model.parameters() if p.requires_grad)}')\n",
    "\n",
    "# Train the GCN model\n",
    "train(gcn_model, dataloader_train, dataloader_val, epochs=12, lr=5e-3, patience=5, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VbtMkpX-u40"
   },
   "source": [
    "After training, you can make predictions using the GCN model. Save the prediction results in a `.txt` file, where each line contains one prediction for one test data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NZNPRFS-u40"
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device=device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            #####\n",
    "            # TODO:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Forward pass with the enhanced model\n",
    "            out = model(batch.x, batch.edge_index, batch.batch)\n",
    "            \n",
    "            # Handle potential shape mismatch (as in training)\n",
    "            if hasattr(batch, 'y') and batch.y.shape[0] != out.shape[0]:\n",
    "                graph_y = []\n",
    "                for i in range(out.shape[0]):\n",
    "                    graph_indices = (batch.batch == i).nonzero(as_tuple=True)[0]\n",
    "                    if len(graph_indices) > 0:  # Make sure there are nodes in this graph\n",
    "                        graph_y.append(batch.y[graph_indices[0]])\n",
    "                # This is only needed for verification if you want to check accuracy\n",
    "                # target = torch.tensor(graph_y, device=device)\n",
    "            \n",
    "            # Get predictions - one per graph\n",
    "            pred = out.argmax(dim=1).cpu().numpy()\n",
    "            predictions.extend(pred)\n",
    "            #####\n",
    "\n",
    "    return predictions\n",
    "\n",
    "dataset_test = GNNBenchmarkDataset(root=data_root, name='CIFAR10', split='test')\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "predictions_gcn = predict(gcn_model, dataloader_test)\n",
    "np.savetxt('predictions_gcn_cifar10.txt', predictions_gcn, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h79ISD2E-u40"
   },
   "source": [
    "## Grading\n",
    "\n",
    "All the tasks will be graded by the accuracy on the test set.\n",
    "\n",
    "### Task 1 (5 points)\n",
    "- Accuracy >= 0.2: 5 points\n",
    "- Accuracy < 0.2: 0 points\n",
    "\n",
    "### Task 2 (10 points)\n",
    "- Accuracy >= 0.32: 10 points\n",
    "- Accuracy < 0.32: 0 points\n",
    "\n",
    "### Task 3 (10 points)\n",
    "- Accuracy >= 0.4: 10 points\n",
    "- Accuracy < 0.4: 0 points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVUuPte1-u40"
   },
   "source": [
    "## Submission\n",
    "\n",
    "After completing all the tasks, you should submit the following four files to Gradescope:\n",
    "\n",
    "- `hw4_gnn.ipynb`: The notebook with all tasks completed.\n",
    "- `predictions_mlp_cluster.txt`: prediction results of the MLP model on CLUSTER dataset.\n",
    "- `predictions_gcn_cluster.txt`: prediction results of the GCN model on CLUSTER dataset.\n",
    "- `predictions_gcn_cifar10.txt`: prediction results of the GCN model on cifar10 dataset.\n",
    "\n",
    "Note that you need to submit the files individually, **DO NOT** submit a zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMa6VVru-u40"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
